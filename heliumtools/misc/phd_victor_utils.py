import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.constants import k as kb
from tqdm import tqdm
from scipy.constants import m_p, hbar
from heliumtools.tools import *
from scipy.optimize import curve_fit
from matplotlib.colors import ListedColormap
from heliumtools.correlations import Correlation
from cycler import cycler
from heliumtools.dataset import Dataset
from heliumtools.correlations import Correlation
from heliumtools.tools import (
    apply_ROI,
    data_filter,
    apply_ROD,
    get_roi_min_max,
    get_roi_center,
)
from heliumtools.misc.some_plots_volume1 import *
from heliumtools.correlations2 import CorrelationHe2Style
from heliumtools.misc.logger import getLogger, logging
from heliumtools.fit.oscillations import Oscillation1DFit
from heliumtools.misc.gather_data import export_data_set_to_pickle
from heliumtools.bec import Gaussian_BEC

m_he = 4 * m_p


def thermal_prof(x, T, Ath):
    return Ath * np.exp(-(x**2) / (2 * T**2))


def phonon_pair(x, x0, sigma, A):
    return A * np.exp(-((x - x0) ** 2) / (2 * sigma**2))


def gaussian_no_offset(x, A, sigma, x0):
    return A * np.exp(-((x - x0) ** 2) / (2 * sigma**2))


def gaussian_with_temp_v2(x, x0_l, sigma_l, A_l, x0_r, sigma_r, A_r, T, Ath, Alas):
    return (
        thermal_prof(x, T, Ath)
        + phonon_pair(x, x0_l, sigma_l, A_l)
        + phonon_pair(x, x0_r, sigma_r, A_r)
        + Alas * np.where(x < 0, 1, 0)
    )


def get_g2(data: pd.DataFrame, axis: str, ROI: dict) -> pd.DataFrame:
    """function that returns the small g² intergrated function along axis.
      The ROI is apllied before integration hence can be understood as integration
      bounds

    Parameters
    ----------
    data : pandas dataframe
        Integrated correlation result dataframe that must contain G2AA, G2AB and G2BB
        and their mean squared and variance. Such dataframe is generated by the
        compute_correlation method as well as the bootstrap one.
    axis : string
        the axis on which one wants to integrate
    ROI : dict
        dictiona

    Returns
    -------
    pd.Dataframe
        g2 integrated over axis
    """
    data = apply_ROI(data, ROI).copy()
    if "G2AA std" in data.columns:
        for G2 in ["G2AA", "G2BB", "G2AB"]:
            data[G2 + " mean squared"] = data[G2 + " mean"] ** 2
            data[G2 + " variance"] = data[G2 + " std"] ** 2
    data = data.groupby(axis).sum().reset_index()
    data_err = data.groupby(axis).mean().reset_index()
    for G2, g2 in zip(["G2AA", "G2BB", "G2AB"], ["g2 aa", "g2 bb", "g2 ab"]):
        data[g2] = data[G2] / data[G2 + " denominator"]
        if G2 + " std" in data.columns:
            data[g2 + " error"] = (
                np.sqrt(data_err[G2 + " squared"] - data_err[G2 + " mean squared"])
                / data[G2 + " denominator"]
            )
            data[g2 + " error bis"] = (
                np.sqrt(data_err[G2 + " variance"]) / data[G2 + " denominator"]
            )
    return data


def fit_pair_density_v3(
    corr, ROI={"Vx": [-50, 50], "Vy": [-50, 50]}, bins=np.arange(5, 20, 0.1), axes=None
):
    bins = np.sort(np.abs(bins))
    # positive peak
    hist_r, bin_edges_r = np.histogram(apply_ROI(corr.atoms, ROI)["Vz"], bins=bins)
    bin_center_r = (bin_edges_r[1:] + bin_edges_r[:-1]) / 2
    # negative peak
    hist_l, bin_edges_l = np.histogram(
        apply_ROI(corr.atoms, ROI)["Vz"], bins=np.sort(-bins)
    )
    bin_center_l = (bin_edges_l[1:] + bin_edges_l[:-1]) / 2
    hist = np.concatenate((hist_l, hist_r))
    bin_center = np.concatenate((bin_center_l, bin_center_r))

    dBin = bins[1] - bins[0]
    hist = hist / dBin / corr.n_cycles
    p0 = [
        bin_center_l[np.argmax(hist_l)],
        1,
        np.max(hist_l),
        bin_center_r[np.argmax(hist_r)],
        1,
        np.max(hist_r),
        8,
        hist[0],
        hist[0],
    ]
    bounds = (
        [-30, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 50, np.inf, 30, 50, np.inf, 200, np.inf, np.inf],
    )
    try:
        popt, pcov = curve_fit(
            gaussian_with_temp_v2, bin_center, hist, p0=p0, bounds=bounds
        )
        perr = np.sqrt(np.diag(pcov))
        T = m_he * (popt[6] / 1000) ** 2 / kb * 1e9  # temperature nK
        dT = float(m_he * (perr[6] / 1000) ** 2 / kb * 1e9)
        res = {
            "Temperature (nK)": float(T),
            "Temperature (mm/s)": float(popt[6]),
            "Temperature error (nK)": dT,
            "Amplitude left (at/mm/s)": float(popt[2]),
            "Width left (mm/s)": float(popt[1]),
            "Width Vz left (mm/s)": float(popt[1]),
            "U(Width Vz left) (mm/s)": float(perr[1]),
            "Position left (mm/s)": float(popt[0]),
            "Amplitude right (at/mm/s)": float(popt[5]),
            "Width right (mm/s)": float(popt[4]),
            "Width Vz right (mm/s)": float(popt[4]),
            "U(Width Vz right) (mm/s)": float(perr[4]),
            "Position right (mm/s)": float(popt[3]),
            "Lasing effect (at/mm/s)": float(popt[8]),
            "Thermal density (at/mm/s)": (
                float(thermal_prof(popt[0], popt[6], popt[7]))
                + float(thermal_prof(popt[3], popt[6], popt[7]))
            )
            / 2,
        }
        # dataset.set(density_positive_peak = T)
    except Exception as e:
        print(f"Failed to fit: {e}")
        popt = p0
        res = {}
        T = np.nan
        return
    try:
        ax = axes[0]
        bins_total = np.arange(-np.max(bins), np.max(bins) + dBin, dBin)
        (
            hist_total,
            b0,
        ) = np.histogram(apply_ROI(corr.atoms, ROI)["Vz"], bins=bins_total)
        hist_total = hist_total / dBin / corr.n_cycles
        ax.plot(
            (bins_total[1:] + bins_total[:-1]) / 2,
            hist_total,
            label="exp",
            color="teal",
        )
        # plot the fit
        for x in [bin_center_l, bin_center_r]:
            ax.plot(
                x,
                gaussian_with_temp_v2(x, *popt),
                label="T = {:.0f} nK".format(T),
                color="indianred",
            )
        ax.set_xlabel("Vz (mm/s)")
        ax.set_ylabel("Atomic density (at/mm/s)")
        ax.set_ylim(top=1.2 * np.max(hist), bottom=0)
    except:
        pass
    dty_color = ["goldenrod", "teal"]
    dty_in_color = ["navajowhite", "skyblue"]
    fit_line = ["indianred", "darkslategrey"]
    markers = ["o", "s"]
    lines = ["--", "-."]
    for j, axis in enumerate(["Vx", "Vy"]):

        for k, peak in enumerate(["left", "right"]):
            data = apply_ROI(
                corr.atoms,
                ROI={
                    "Vz": {
                        "center": res[f"Position {peak} (mm/s)"],
                        "size": 2 * res[f"Width {peak} (mm/s)"],
                    }
                },
            )
            hist, bins = np.histogram(data[axis], np.arange(-35, 35, 2))
            y = hist / np.max(hist)
            x = bins[0:-1]
            ### fit
            func = gaussian_no_offset
            popt, pcov = curve_fit(func, x, y, p0=[1, 5, 0])
            perr = np.sqrt(np.diag(pcov))

            res[f"Width {axis} {peak} (mm/s)"] = float(popt[1])
            res[f"U(Width {axis} {peak}) (mm/s)"] = float(perr[1])
            res[f"Center {axis} {peak} (mm/s)"] = float(popt[2])

            #######
            ## PLOT
            #######
            try:
                ax = axes[j + 1]
                ax.plot(
                    x,
                    y,
                    marker=markers[k],
                    markerfacecolor=dty_in_color[k],
                    markeredgecolor=dty_color[k],
                    markersize=4,
                    alpha=1,
                    ls="",
                )
                ax.set_xlabel(axis + " (mm/s)")

                x_fit = np.linspace(min(x), max(x), 100)
                ax.plot(
                    x_fit,
                    func(x_fit, *popt),
                    ls=lines[k],
                    color=fit_line[k],
                    label=r"{:.1f}({:.0f})".format(np.abs(popt[1]), 10 * perr[1]),
                )
                ax.legend(
                    ncol=1,
                    loc="lower center",
                    fontsize=8,
                    title="$\sigma$ (mm/s)",
                    title_fontsize=8,
                )
            except:
                pass
    return res


def fit_pair_density_v2(
    corr, ROI={"Vx": [-50, 50], "Vy": [-50, 50]}, bins=np.arange(5, 20, 0.1), ax=None
):
    bins = np.sort(np.abs(bins))
    # positive peak
    hist_r, bin_edges_r = np.histogram(apply_ROI(corr.atoms, ROI)["Vz"], bins=bins)
    bin_center_r = (bin_edges_r[1:] + bin_edges_r[:-1]) / 2
    # negative peak
    hist_l, bin_edges_l = np.histogram(
        apply_ROI(corr.atoms, ROI)["Vz"], bins=np.sort(-bins)
    )
    bin_center_l = (bin_edges_l[1:] + bin_edges_l[:-1]) / 2
    hist = np.concatenate((hist_l, hist_r))
    bin_center = np.concatenate((bin_center_l, bin_center_r))

    dBin = bins[1] - bins[0]
    hist = hist / dBin / corr.n_cycles
    p0 = [
        bin_center_l[np.argmax(hist_l)],
        1,
        np.max(hist_l),
        bin_center_r[np.argmax(hist_r)],
        1,
        np.max(hist_r),
        8,
        hist[0],
        hist[0],
    ]
    bounds = (
        [-30, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 50, np.inf, 30, 50, np.inf, 200, np.inf, np.inf],
    )
    try:
        popt, pcov = curve_fit(
            gaussian_with_temp_v2, bin_center, hist, p0=p0, bounds=bounds
        )
        perr = np.sqrt(np.diag(pcov))
        T = m_he * (popt[6] / 1000) ** 2 / kb * 1e9  # temperature nK
        dT = float(m_he * (perr[6] / 1000) ** 2 / kb * 1e9)
        res = {
            "Temperature (nK)": float(T),
            "Temperature (mm/s)": float(popt[6]),
            "Temperature error (nK)": dT,
            "Amplitude left (at/mm/s)": float(popt[2]),
            "Width left (mm/s)": float(popt[1]),
            "Position left (mm/s)": float(popt[0]),
            "Amplitude right (at/mm/s)": float(popt[5]),
            "Width right (mm/s)": float(popt[4]),
            "Position right (mm/s)": float(popt[3]),
            "Lasing effect (at/mm/s)": float(popt[8]),
            "Thermal density (at/mm/s)": (
                float(thermal_prof(popt[0], popt[6], popt[7]))
                + float(thermal_prof(popt[3], popt[6], popt[7]))
            )
            / 2,
        }
        # dataset.set(density_positive_peak = T)
    except Exception as e:
        print(f"Failed to fit: {e}")
        popt = p0
        res = {}
        T = np.nan
    if ax:
        bins_total = np.arange(-np.max(bins), np.max(bins) + dBin, dBin)
        (
            hist_total,
            b0,
        ) = np.histogram(apply_ROI(corr.atoms, ROI)["Vz"], bins=bins_total)
        hist_total = hist_total / dBin / corr.n_cycles
        ax.plot(
            (bins_total[1:] + bins_total[:-1]) / 2,
            hist_total,
            label="exp",
            color="teal",
        )
        # plot the fit
        for x in [bin_center_l, bin_center_r]:
            ax.plot(
                x,
                gaussian_with_temp_v2(x, *popt),
                label="T = {:.0f} nK".format(T),
                color="indianred",
            )
        ax.set_xlabel("Vz (mm/s)")
        ax.set_ylabel("Atomic density (at/mm/s)")
        ax.set_ylim(top=1.2 * np.max(hist), bottom=0)
    return res


def get_correlation_many(
    corr,
    N_bootstrap=10,
    Vz_range=[7, 12],
    box_Z_size=[0.3, 0.5, 0.7, 0.9],
    box_XY_size=[80],
    ratio_size={"Vx": 1, "Vy": 1, "Vz": 1},
    DV=0.8,
):
    """fonction qui retourne un gros dataframe avec les propriétés de corrélations pour différentes positions de boîtes Vz1 et Vz2. La fonction commence par récupérer le maximum de densité croisée pour ensuite calculer les corrélations autour de celle-ci (dans un espace DV autour).


    Parameters
    ----------
    path : _type_
        _description_
    N_bootstrap : int, optional
        _description_, by default 10

    Returns
    -------
    _type_
        _description_
    """
    COLUMNS = [
        "Vz1",
        "Vz2",
        "g^2",
        "N_1",
        "N_2",
        "g^2(k2,k2)",
        "g^2(k1,k1)",
        "N_1*N_2",
        "normalized variance",
        "g^4",
        "C-S difference",
        "denis",
        "denis2",
        "Delta",
        "-Delta",
    ]
    TOTAL = []
    # la taille qu'on s'autorise pour aller chercher des corrélations
    corr.boxes["1"] = {
        "Vz": {"position": -9.2, "size": 1},
        "Vx": {"position": 0, "size": 80},
        "Vy": {"position": 0, "size": 80},
    }
    corr.boxes["2"] = {
        "Vz": {"position": 9.2, "size": 1},
        "Vx": {"position": 0, "size": 80},
        "Vy": {"position": 0, "size": 80},
    }
    corr.define_variable1(
        box="1",
        axe="Vz",
        type="position",
        name="Vz1",
        min=-np.max(Vz_range),
        max=-np.min(Vz_range),
        step=0.1,
    )
    corr.define_variable2(
        box="2",
        axe="Vz",
        type="position",
        name="Vz2",
        min=np.min(Vz_range),
        max=np.max(Vz_range),
        step=0.1,
    )
    corr.remove_shot_noise = False
    corr.compute_correlations()
    # on récupère le maximum de corrélation non normalisée
    serie = corr.result.loc[np.argmax(corr.result["N_1*N_2"])]
    Vz1 = serie.loc["Vz1"]
    Vz2 = serie.loc["Vz2"]
    for boxZsize in tqdm(box_Z_size):
        for boxXYsize in box_XY_size:

            # we now loop over the difference situations
            corr.boxes["1"]["Vz"]["size"] = boxZsize * ratio_size["Vz"]
            corr.boxes["1"]["Vx"]["size"] = boxXYsize * ratio_size["Vx"]
            corr.boxes["1"]["Vy"]["size"] = boxXYsize * ratio_size["Vy"]
            corr.boxes["2"]["Vz"]["size"] = boxZsize
            corr.boxes["2"]["Vx"]["size"] = boxXYsize
            corr.boxes["2"]["Vy"]["size"] = boxXYsize

            corr.define_variable1(
                box="1",
                axe="Vz",
                type="position",
                name="Vz1",
                min=Vz1 - DV,
                max=Vz1 + DV,
                step=0.1,
            )
            corr.define_variable2(
                box="2",
                axe="Vz",
                type="position",
                name="Vz2",
                min=Vz2 - DV,
                max=Vz2 + DV,
                step=0.1,
            )
            corr.compute_correlations()
            res = []
            final_df = corr.result.loc[:, COLUMNS]
            corr.save_copy_of_total()
            for kboot in range(N_bootstrap):
                corr.bootstrap_total()
                corr.compute_result(corr.total)
                res.append(corr.result)
            res = pd.concat(res)
            res_mean = res.groupby(["Vz1", "Vz2"]).mean().reset_index()
            res_err = res.groupby(["Vz1", "Vz2"]).std().reset_index()
            for key in [
                "g^2",
                "N_1",
                "N_2",
                "g^2(k2,k2)",
                "g^2(k1,k1)",
                "N_1*N_2",
                "normalized variance",
                "g^4",
                "C-S difference",
                "denis",
                "denis2",
                "Delta",
                "-Delta",
            ]:
                final_df["Mean " + key] = res_mean[key]
                final_df["U(" + key + ")"] = res_err[key]
            final_df["boxXYsize"] = boxXYsize
            final_df["boxZsize"] = boxZsize
            final_df["boxZsize1"] = boxZsize
            final_df["boxZsize2"] = boxZsize
            TOTAL.append(final_df)
    return pd.concat(TOTAL).reset_index(drop=True)


def show_vz_correlations(corr, VXY_list=[100, 50, 8], VZ_span=[-12, 12]):
    """function that take as an input a IntegratedCorrelation class and show the integrated g2 along z

    Parameters
    ----------
    corr : CorrelationHe2Style
        Correlation already calculated.
    VXY_list : list, optional
        transverse integration volume list for all plots, by default [100, 50, 8]
    VZ_span : list, optional
        longitudinal ROI, that are the span of the axes., by default [-12, 12]
    """
    fig, axes = plt.subplots(
        figsize=(11, 4),
        ncols=3,
    )
    axis = "Vz"
    # -- Set style
    custom_cycler1 = cycler(
        color=[plt.get_cmap("magma")(i) for i in range(0, 256, 256 // len(VXY_list))]
    )
    axes[2].set_prop_cycle(custom_cycler1)
    custom_cycler2 = cycler(
        color=[plt.get_cmap("viridis")(i) for i in range(0, 256, 256 // len(VXY_list))]
    )
    axes[0].set_prop_cycle(custom_cycler2)
    axes[1].set_prop_cycle(custom_cycler2)
    markers = ["o", "v", "s", "d", "p", "*", "H", "P", "<", "+"] * 3
    lines = ["--", "-.", ":"] * 10
    for j, vxy in enumerate(VXY_list):
        ROI = {
            "Vx": {"position": 0, "size": 2 * vxy},
            "Vy": {"position": 0, "size": 2 * vxy},
            "Vz": VZ_span,
        }
        for i, TO_PLOT in enumerate(["g2 aa", "g2 bb", "g2 ab"]):
            ax = axes[i]
            df = get_g2(corr.result, axis, ROI)
            x = df[axis]
            y = df[TO_PLOT]
            yerr = df[TO_PLOT + " error"]
            ax.errorbar(
                x, y, yerr=yerr, fmt=markers[j], label="{}".format(vxy), alpha=0.7
            )
    for ax in axes:
        ax.legend(fontsize="small", title=r"$\delta V_\perp$")
        ax.grid(True, alpha=0.5)
        ax.set_xlabel("$\delta {} = k_{{Z,1}} -k_{{Z,2}}  $ (mm/s)".format(axis))
        ax.set_ylabel("$\int g^{{(2)}}(k, k+\delta k_z) dk_z$")
    axes[2].set_xlabel("$\delta {} = k_{{Z,1}} + k_{{Z,2}}  $ (mm/s)".format(axis))

    ## Style
    if corr.cross_correlation_sign["Vx"] > 0:
        sign = "+"
    else:
        sign = "-"
    title = f"Crossed correlations function (sign:${sign}$) \n in inertial frame "
    title += r"$\vec{{V}}$=({}, {}, {}) mm/s ".format(
        corr.ref_frame_speed["Vx"],
        corr.ref_frame_speed["Vy"],
        corr.ref_frame_speed["Vz"],
    )
    axes[2].set_title(title, fontsize="medium")

    if get_roi_center(corr.beams["A"], "Vz") > 0:
        title = r" ($V_z > 0$)."
    else:
        title = r" ($V_z < 0$)."
    axes[0].set_title("Beam A local correlations" + title, fontsize="medium")
    if get_roi_center(corr.beams["B"], "Vz") > 0:
        title = r" ($V_z > 0$)."
    else:
        title = r" ($V_z < 0$)."
    axes[1].set_title("Beam B local correlations" + title, fontsize="medium")
    plt.tight_layout()
    plt.show()


import matplotlib


def get_centered_colormap(
    colormap_name: str, mini: float, maxi: float, center: float
) -> matplotlib.colormaps:
    """function that returns a colormap centered on center and with boundaries mini and maxi.

    Parameters
    ----------
    colormap_name : str
        name of the colormap
    mini : float
        minimum value for the colormap
    maxi : float
        maximum value for the colormap
    center : float
        center of the colormap

    Returns
    -------
    matplotlib.colormaps
        corlorma centered and beatifull for your graph <3
    """

    from matplotlib import cm
    from matplotlib.colors import ListedColormap, LinearSegmentedColormap

    scale = maxi - mini
    ratio_mini = (center - mini) / scale
    ratio_maxi = (maxi - center) / scale
    if ratio_mini > ratio_maxi:
        np_maxi = 130
        np_mini = int(np_maxi * ratio_mini / ratio_maxi)
    else:
        np_mini = 130
        np_maxi = int(np_mini * ratio_maxi / ratio_mini)
    cmap_mini = plt.get_cmap(colormap_name, 2 * np_mini)
    mini_val = cmap_mini(np.linspace(0, 1, 2 * np_mini))[0:np_mini]
    cmap_maxi = plt.get_cmap(colormap_name, 2 * np_maxi)
    maxi_val = cmap_maxi(np.linspace(0, 1, 2 * np_maxi))[np_maxi:-1]
    my_colormap = ListedColormap(list(mini_val) + list(maxi_val))
    return my_colormap


def sinus_card(x, A, sigma, x0):
    return A * np.sinc((np.abs((x - x0) / (2 * sigma))) ** (1)) + 1


def sinus_card_sqrt(x, A, sigma, x0):
    return A * np.sinc((np.abs((x - x0) / (2 * sigma))) ** (1 / 2)) + 1


def lorentzian(x, A, sigma, x0):
    return 1 + A * (1 / (1 + (x - x0) ** 2 / (sigma) ** 2))


def lorentzian2(x, A, sigma, x0):
    return 1 + A * (1 / (1 + (x - x0) ** 2 / (sigma) ** 2) ** 2)


def gaussian(x, A, sigma, x0):
    return 1 + A * np.exp(-((x - x0) ** 2) / (2 * sigma**2))


def expo(x, A, sigma, x0):
    return 1 + A * np.exp(-np.abs(x - x0) / sigma)


class CorrelationKmK(Correlation):
    def compute_kmk_correlations(self, dk_optimisation=0.3, optimizer="N_1*N_2"):
        """Function that compute the corration between k and -k.


        Parameters
        ----------
        dk_optimisation : float, optional
            range to optimize, by default 0.3
        optimizer : str, optional
            _description_, by default "N_1*N_2"
        """
        self.compute_correlations()

        self.filter_result_kmk(dk_optimisation=dk_optimisation, optimizer=optimizer)

    def filter_result_kmk(self, dk_optimisation=0.3, optimizer="N_1*N_2"):
        """filter out the result dataframe to get only (k,-k) correlations.

        Parameters
        ----------
        dk_optimisation : float, optional
            range to optimize, by default 0.3
        optimizer : str, optional
            _description_, by default "N_1*N_2"
        """
        self.result = apply_ROI(
            self.result,
            ROI={
                f"{self.var1.name}+{self.var2.name}": [
                    -dk_optimisation,
                    dk_optimisation,
                ]
            },
        )
        self.result = self.result.sort_values(
            optimizer, ascending=False
        ).drop_duplicates([f"{self.var1.name}-{self.var2.name}"])
        self.result["k"] = self.result[f"{self.var1.name}-{self.var2.name}"] / 2
        self.result = self.result.sort_values("k")

    def set_size_with_ratio(
        self, dVz, dVx, dVy, ratio_size={"Vx": 1, "Vy": 1, "Vz": 1}
    ):
        """set the boxes size taking itnot account the discrepancy between the size of the mode.

        Parameters
        ----------
        dVz : float
            size of the box along Vz (box 2 size (right peak))
        dVx : float
            size of the box along Vx (box 2 size (right peak))
        dVy : float
            size of the box along Vy (box 2 size (right peak))
        ratio_size : dict, default {"Vz":1, "Vy":1, "Vz":1}
            ratio of the size
        """
        self.boxes["1"]["Vz"]["size"] = dVz * ratio_size["Vz"]
        self.boxes["1"]["Vx"]["size"] = dVx * ratio_size["Vx"]
        self.boxes["1"]["Vy"]["size"] = dVy * ratio_size["Vy"]
        self.boxes["2"]["Vz"]["size"] = dVz
        self.boxes["2"]["Vx"]["size"] = dVx
        self.boxes["2"]["Vy"]["size"] = dVy


from scipy.special import factorial, gamma
from heliumtools.tools import apply_ROI, data_filter, get_roi_size

# import thewalrus

from scipy.special import factorial, gamma
from heliumtools.tools import apply_ROI, data_filter, get_roi_size


def thermal_distribution(x, moy):
    """Définie une loi de probabilité thermique via la moyenne de la loi"""
    return (moy / (1 + moy)) ** x / (1 + moy)


def poissonian_distribution(x, moy):
    """Définie une loi de probabilité poissonnienne via la moyenne de la loi"""
    return np.exp(-moy) * moy**x / factorial(x)


def fit_multimode_thermal_distribution(x, px, moy, perr=None):
    """Fit une loi de probabilité thermique multimode. Le seul paramètre libre de
    fit est le nombre de mode. La population moyenne étant 'mesurée'. C'est pour
    cela que la fonction est loi de probabilité multimmode thermique est définie
    dans cette fonction et non en dehors."""

    def proba_multimode(n, n_modes):
        """Définie une loi de probabilité multimode thermique via la moyenne
        (censé être un argument mais gardé fixe ici pour le fit), et le nombre de modes.
        """
        return (
            gamma(n + n_modes)
            / (gamma(n + 1) * gamma(n_modes))
            * (1 + n_modes / moy) ** (-n)
            * (1 + moy / n_modes) ** (-n_modes)
        )

    popt, pcov = curve_fit(proba_multimode, x, px, p0=[1], bounds=(0.1, 30))
    perr = np.sqrt(np.diag(pcov))
    return popt[0], perr[0], proba_multimode(x, *popt)


def proba_multimode_plot(n, n_modes, moy):
    """Définie une loi de probabilité multimode thermique via la moyenne
    (censé être un argument mais gardé fixe ici pour le fit), et le nombre de modes."""
    return (
        gamma(n + n_modes)
        / (gamma(n + 1) * gamma(n_modes))
        * (1 + n_modes / moy) ** (-n)
        * (1 + moy / n_modes) ** (-n_modes)
    )


def get_proba_distribution(correl, box_num="1"):
    """return the probability distribution of one of the box of the atoms from the class Correlations correl

    Parameters
    ----------
    correl : Correlation
        the correlation class from which one wants the probability distribution
    box_num : str, optional "1" or "2"
        box number of the correlation class , by default "1"
    """
    at = correl.obtain_number_of_atoms_per_cycle_in_box(
        correl.atoms, correl.boxes[box_num], "N"
    )
    at["Probability"] = np.ones(len(at))
    # on définie df, dataframe qui donne le nombre d'atome
    df = at.groupby("N").count().reset_index()
    df["Probability"] = df["Probability"] / np.sum(df["Probability"])
    df["Probability error"] = np.sqrt(df["Probability"]) / np.sqrt(len(at))
    mean_at_num = np.sum(df["Probability"] * df["N"])
    mean_at_num2 = np.sum(at["N"]) / len(at)
    if np.abs(mean_at_num - mean_at_num2) > 1e-8:
        print("Something is wrong with the mean in the calculation.")
    return mean_at_num2, df


class CorrelationFCS(Correlation):
    def __init__(self, atoms, **kwargs):
        """
        Object initialization, sets parameters as the user defined, build the atoms dataframe and apply ROD and ROI.
        """
        super().__init__(atoms, **kwargs)

    def compute_fcs(self, N_boostrap=10, correlation_max=4):
        self.var2 = None
        self.compute_correlations()
        ### choose only some columns
        self.total = self.total[["Cycle", "N_1", self.var2.name, self.var1.name, "N_2"]]
        self.probability = self.get_probability(correlation_max=correlation_max)
        self.gn = self.get_gn(self.probability)
        self.save_copy_of_total()
        bootstrap_result = []
        for j in range(N_boostrap):
            self.bootstrap_total()
            proba = self.get_probability(correlation_max=correlation_max)
            gn = self.get_gn(proba)
            gn["Bootstrap"] = j
            bootstrap_result.append(gn)
        bootstrap_result = pd.concat(bootstrap_result)
        bootstrap_result = (
            bootstrap_result.groupby([self.var1.name, "order n"]).std().reset_index()
        )
        for col in bootstrap_result.columns:
            if col not in [self.var1.name, "order n"]:
                bootstrap_result.rename(columns={col: f"U({col})"}, inplace=True)
        self.gn = self.gn.merge(bootstrap_result, on=[self.var1.name, "order n"])

    def get_gn(self, probability):
        exist = True
        i = 1
        g_n = []
        probability["Nmean"] = probability["Probability"] * probability["N_1"]
        while exist:
            try:
                probability[f"G^n"] = (
                    probability["Probability"] * probability[f":n^{i}:"]
                )
                new_gn = (
                    probability.groupby(self.var1.name)
                    .sum()[["G^n", "Nmean"]]
                    .reset_index()
                )
                new_gn["order n"] = i
                new_gn["g^n"] = new_gn["G^n"] / new_gn["Nmean"] ** i
                new_gn["g^nth"] = factorial(i)
                g_n.append(new_gn)
            except Exception as e:
                exist = False
            i += 1

        return pd.concat(g_n)

    def get_probability(self, correlation_max=4):
        probability = (
            self.total.groupby(["N_" + self.var1.box, self.var1.name])
            .count()[["Cycle"]]
            .reset_index()
            .rename(columns={"Cycle": "Probability"})
        )
        probability["Probability"] = probability["Probability"] / self.n_cycles
        gn_list = []
        probability[":n^1:"] = probability["N_1"]
        for i in range(2, correlation_max + 1):
            probability[f":n^{i}:"] = probability[f":n^{i-1}:"] * (
                probability["N_1"] - i + 1
            )

        return probability


def add_FCS_on_ax(
    correl,
    ax,
    box_num="1",
    color="goldenrod",
    style="o",
    poisson="teal",
    thermal="indianred",
):
    """Function to show full counting statistics"""
    mean, df = get_proba_distribution(correl, box_num=box_num)
    ### AJOUT DE L'INSET
    L, W = 0.5, 0.5
    ax_ins = ax.inset_axes([1 - L, 1 - W, L, W])
    # n_modes, deltan_modes, pth = fit_multimode_thermal_distribution(df["N"].to_numpy(),
    #                                                               df["Probability"].to_numpy(),
    #                                                               moy,
    #                                                               perr = df["Probability error"].to_numpy())

    x = np.linspace(0, np.max(df["N"]) + 2, 300)
    # pth_plot = proba_multimode_plot(x,n_modes ,moy)
    for a in [ax, ax_ins]:
        dN_th = np.sqrt(mean**2 + mean) / mean / np.sqrt(correl.n_cycles)
        a.plot(x, thermal_distribution(x, mean), color=thermal, alpha=1)
        ax.fill_between(
            x,
            thermal_distribution(x, mean - dN_th),
            thermal_distribution(x, mean + dN_th),
            color=thermal,
            alpha=0.1,
        )
        a.plot(x, poissonian_distribution(x, mean), color="teal", ls="--")
        dN_p = np.sqrt(mean) / mean / np.sqrt(correl.n_cycles)
        ax.fill_between(
            x,
            poissonian_distribution(x, mean - dN_p),
            poissonian_distribution(x, mean + dN_p),
            color=poisson,
            alpha=0.1,
        )
        a.errorbar(
            df["N"],
            df["Probability"],
            yerr=df["Probability error"],
            fmt=style,
            color=color,
            markeredgecolor=color,
            markerfacecolor="#ffffff00",
            label=r"$\bar N =${:.1f}".format(mean),
        )
        a.grid(True, alpha=0.5)
    print(dN_th)
    # ax.plot(x,pth_plot, label = r"$\bar M_{{modes}}=${:.1f}({})".format(n_modes, round(10*deltan_modes)), ls = "--", color = "black", alpha = 0.7)
    df["PobaErr2"] = np.sqrt(thermal_distribution(df["N"], mean) / correl.n_cycles)

    # ax_ins.errorbar(df["N"], df["Probability"], yerr = df["PobaErr2"],
    #             fmt = "o", color = "steelblue", markerfacecolor= "lightblue",
    #             markersize = 4,
    #             label = r"$\bar N =${:.1f}".format(mean), alpha = 1)
    ax_ins.set_xlim([-0.3, max(3 * mean, 3.3)])
    ytick = list(np.arange(0, ax.get_ylim()[1], 0.1))
    ytick_lab = [str(round(i, 1)) for i in ytick]
    ytick_lab[0] = "0"
    ax_ins.set_yticks(ytick)
    ax_ins.set_yticklabels(ytick_lab, fontsize="small")
    ax.set_yscale("log")
    # ax_ins.set_yticklabels(ax_ins.get_yticks(), fontsize='small')
    print(ax_ins.get_ylim())

    # ax_ins.set_xticks(ax.get_xticks())
    ax.set_ylim(bottom=0.5 * np.min(df["Probability"]))


def show_correlations_heatmaps(corr):
    nrows = 2
    ncols = 4
    fig, axes = plt.subplots(figsize=(4 * ncols, 3.3 * nrows), nrows=nrows, ncols=ncols)
    axes = axes.flatten()
    CMAP_PROP = {
        "g^2": {"cmap": "BrBG", "vmin": 1, "vmax": 2.3, "center": 2},
        "normalized variance": {"cmap": "seismic", "vmin": 0.8, "vmax": 2, "center": 1},
        "C-S": {"cmap": "seismic", "vmin": 0.5, "vmax": 1.1, "center": 1},
        "g^2 loc": {"cmap": "BrBG", "vmin": 1, "vmax": 2.3, "center": 2},
        "C-S difference": {
            "cmap": "seismic",
            "vmin": corr.result["C-S difference"].min(),
            "vmax": corr.result["C-S difference"].max(),
            "center": 0,
        },
    }
    corr.result["g^2 loc"] = (
        0.5 * corr.result["g^2(k1,k1)"] + 0.5 * corr.result["g^2(k2,k2)"]
    )
    corr.result.loc[corr.result["g^2"] == np.nan, "g^2"] = 1
    corr.result.loc[corr.result["C-S"] == np.nan, "C-S"] = 1
    corr.result.loc[corr.result["C-S difference"] == np.nan, "C-S difference"] = 0
    heatmap_with_boxes(
        ax=axes[0],
        df=corr.result,
        boxes={},
        columns=corr.var1.name,
        index=corr.var2.name,
        values="N_1*N_2",
        cmap="viridis",
    )
    axes[0].set_title(f"N1*N2", fontsize="medium")
    for j, TO_show in enumerate(
        ["g^2", "C-S", "normalized variance", "g^2 loc", "C-S difference"]
    ):
        ax = axes[j + 1]
        cmap = get_centered_colormap(
            CMAP_PROP[TO_show]["cmap"],
            CMAP_PROP[TO_show]["vmin"],
            CMAP_PROP[TO_show]["vmax"],
            CMAP_PROP[TO_show]["center"],
        )
        cmap.set_bad("grey")
        heatmap_with_boxes(
            ax=ax,
            df=corr.result,
            boxes={},
            columns=corr.var1.name,
            index=corr.var2.name,
            values=TO_show,
            vmax=CMAP_PROP[TO_show]["vmax"],
            vmin=CMAP_PROP[TO_show]["vmin"],
            cmap=cmap,
        )
        ax.set_title(TO_show)

    df1 = corr.result[corr.result[corr.var1.name] == corr.result[corr.var1.name].min()]
    df2 = corr.result[corr.result[corr.var2.name] == corr.result[corr.var2.name].min()]
    ax = axes[-2]
    ax.set_xlabel("|Vz| (mm/s)")
    ax.set_ylabel("$g^{(2)}_{k,k}$")

    ax.plot(-df2[corr.var1.name], df2["g^2(k1,k1)"], label="-")
    ax.plot(df1[corr.var2.name], df1["g^2(k2,k2)"], label="+")
    ax = axes[-1]
    ax.plot(-df2[corr.var1.name], df2["N_1"], label="-")
    ax.plot(df1[corr.var2.name], df1["N_2"], ls="--", label="+")
    ax.set_xlabel("|Vz| (mm/s)")
    ax.set_ylabel("Population")
    for ax in axes[0:-2]:
        ax.plot(np.flip(corr.var1.values), corr.var2.values, ls="--", color="white")
    fig.suptitle(
        "Arrival time : {:.4f} ms and inertial frame: {}".format(
            corr.theoretical_arrival_time, corr.ref_frame_speed
        )
    )
    plt.tight_layout()
    plt.show()


def show_n_order_correlation(corr):
    """show the nth order correation function and the full counting statitstics of
    a correlation with only 1 variable.

    Parameters
    ----------
    corr : _type_
        _description_
    """
    corr.define_variable1(
        box="1",
        axe="Vz",
        type="position",
        name="Vz1",
        values=[corr.boxes["1"]["Vz"]["position"]],
    )
    corr.define_variable2(
        box="2",
        axe="Vz",
        type="position",
        name="Vz2",
        values=[corr.boxes["2"]["Vz"]["position"]],
    )
    corr.compute_correlations()
    corr.save_copy_of_total()
    df = []
    for n in range(50):
        corr.bootstrap_total()
        corr.compute_result(corr.total)
        df.append(corr.result)
    df = pd.concat(df)
    df["g_1^(1)"] = 1
    df["g_2^(1)"] = 1
    corr.result = df.groupby(["Vz1", "Vz2"]).mean().reset_index()

    corr.error = df.groupby(["Vz1", "Vz2"]).std().reset_index()
    fig, axes = plt.subplots(ncols=4, figsize=(12, 3))
    # color = color_l[i],style = markers[i]
    add_FCS_on_ax(corr, axes[0], box_num="1", color="tan")
    add_FCS_on_ax(corr, axes[1], box_num="2", color="tan")
    axes[0].legend(loc=3)
    axes[1].legend(loc=3)
    axes[0].set_xlabel("Number of atoms")
    axes[1].set_xlabel("Number of atoms")

    df = pd.DataFrame({"n": np.arange(1, corr.correlation_order_max + 1, 1)})

    for i in range(len(df)):
        n = df.loc[i, "n"]
        for j in range(1, 3):

            df.loc[i, f"g_{j}^(n)"] = corr.result.loc[0, f"g_{j}^({n})"]
            df.loc[i, f"U(g_{j}^(n))"] = float(corr.error.loc[0, f"g_{j}^({n})"])
            df.loc[i, f"g_{j}^(n)/n!"] = corr.result.loc[0, f"g_{j}^({n})"] / factorial(
                n
            )
            df.loc[i, f"U(g_{j}^(n)/n!)"] = float(
                corr.error.loc[0, f"g_{j}^({n})"]
            ) / factorial(n)

    n_vector = np.arange(1, corr.correlation_order_max + 0.1, 1)
    nfactor = factorial(n_vector)
    ax = axes[2]
    ### zoom
    decalage = 1 / 7
    size = 0.12
    sizeh = 0.3
    markers = "os<>"
    axin_l = [
        ax.inset_axes([0.17, 0.3, size, sizeh]),
        ax.inset_axes([0.35, 0.65, size, sizeh]),
    ]
    colors = ["teal", "orange"]
    labels = ["-", "+"]
    for i in range(1, 3):
        ax.errorbar(
            df["n"] - (i - 1.5) * decalage,
            df[f"g_{i}^(n)"],
            yerr=df[f"U(g_{i}^(n))"],
            fmt=markers[i],
            color=colors[i - 1],
            label=labels[i - 1],
            markerfacecolor="#ffffff00",
            markeredgecolor=colors[i - 1],
        )
        axes[3].errorbar(
            df["n"] - (i - 1.5) * decalage,
            df[f"g_{i}^(n)/n!"],
            yerr=df[f"U(g_{i}^(n)/n!)"],
            fmt=markers[i],
            color=colors[i - 1],
            label=labels[i - 1],
            markerfacecolor="#ffffff00",
            markeredgecolor=colors[i - 1],
        )
    ax.legend(loc=4)
    ## theory
    ax.plot(n_vector, nfactor, color="indianred", label="n!")
    for k, axin in enumerate(axin_l):
        axin.set_xlim([k + 2 - 0.3, k + 2 + 0.3])
        axin.set_ylim([factorial(k + 2) * (1 - 0.2), factorial(k + 2) * (1 + 0.2)])
        for i in range(1, 3):
            axin.errorbar(
                df["n"] - (i - 1.5) * decalage,
                df[f"g_{i}^(n)"],
                yerr=df[f"U(g_{i}^(n))"],
                fmt=markers[i],
                color=colors[i - 1],
                markerfacecolor="#ffffff00",
                markeredgecolor=colors[i - 1],
            )
        axin.plot(n_vector, nfactor, color="indianred", label="n!")
        axin.set_yticklabels(np.round(axin.get_yticks(), 2), fontsize="small")
        axin.set_xticks([])
        axin.grid(True, alpha=0.5)
        ax.indicate_inset_zoom(axin)
    ax.grid(True, alpha=0.5)
    axes[3].grid(True, alpha=0.5)
    ax.set_yscale("log")
    plt.show()
    print(corr.result[["g^2", "C-S", "N_1", "N_2", "normalized variance"]])
    print(corr.error[["g^2", "C-S", "N_1", "N_2", "normalized variance"]])


def show_correlation_kmk_optimized(
    corr, range_l=(10, 12), optimizer="g^2", show=True, DV=0.3
):

    corr.define_variable1(
        box="1",
        axe="Vz",
        type="position",
        name="Vz1",
        min=-max(range_l),
        max=-min(range_l),
        step=0.1,
    )
    corr.define_variable2(
        box="2",
        axe="Vz",
        type="position",
        name="Vz2",
        min=min(range_l),
        max=max(range_l),
        step=0.1,
    )
    corr.compute_correlations()
    res_mean0 = corr.result
    corr.save_copy_of_total()
    res = []
    for i in tqdm(range(30)):
        corr.bootstrap_total()
        corr.compute_result(corr.total)
        res.append(corr.result)
    res = pd.concat(res)
    res_std = res.groupby([corr.var1.name, corr.var2.name]).std().reset_index()
    # res_mean = res.groupby([corr.var1.name, corr.var2.name]).mean().reset_index()
    res_mean = res_mean0
    for col in res_std.columns:
        res_mean[f"U({col})"] = res_std[col]
        # res_mean[f"{col} for opti"] = res_mean[col]-  res_mean[f"U({col})"]
    res_mean = apply_ROI(res_mean, {"Vz1+Vz2": {"position": 0, "size": DV}})
    df = (
        res_mean.sort_values(optimizer, ascending=False)
        .drop_duplicates([f"Vz1-Vz2"])
        .reset_index(drop=True)
    )
    if show:
        fig, axes = plt.subplots(ncols=6, nrows=1, figsize=(5 * 3, 2.8))
        for j, col in enumerate(["N_1*N_2", "Delta", "g^2", "C-S", "g^4"]):
            ax = axes[j]
            ax.errorbar(np.abs(df["(Vz1-Vz2)/2"]), df[col], yerr=df[f"U({col})"], ls="")
            ax.set_ylabel(col)
            ax.grid(True, alpha=0.5)
            ax.set_xlabel("$|v_z|$ (mm/s)")
            # ax.set_xlim([8,10])
        for col in ["g^2(k1,k1)", "g^2(k2,k2)"]:
            axes[-1].errorbar(
                np.abs(df["(Vz1-Vz2)/2"]), df[col], yerr=df[f"U({col})"], ls=""
            )
            axes[-1].grid(True, alpha=0.5)
            ax.set_xlabel("$|v_z|$ (mm/s)")
            ax.set_ylabel("$g^{(2)}(k,k)$")
            ax.legend(["-k", "+k"])
        plt.tight_layout()
        plt.show()
    return res_mean
